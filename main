import sys
import json
import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType

def load_config(file_path):
    """
    Load configuration file from S3.
    """
    s3 = boto3.client('s3')
    bucket_name = file_path.split('/')[2]
    key = "/".join(file_path.split('/')[3:])
    
    obj = s3.get_object(Bucket=bucket_name, Key=key)
    return json.loads(obj['Body'].read().decode('utf-8'))

def parse_schema(schema_str):
    """
    Convert a string-based schema into a PySpark StructType schema.
    """
    if not schema_str:
        return None
    
    fields = []
    for field in schema_str.split(","):
        field_name, field_type = field.strip().rsplit(maxsplit=1)
        if field_type.upper() == "INT":
            fields.append(StructField(field_name, IntegerType(), True))
        elif field_type.upper() == "LONG":
            fields.append(StructField(field_name, LongType(), True))    
        elif field_type.upper() == "STRING":
            fields.append(StructField(field_name, StringType(), True))
    
    return StructType(fields)

def read_data(glueContext, config):
    """
    Read data from various file formats based on the config file.
    """
    source_path = config['source_path']
    source_type = config['source_type']
    schema_str = config.get('schema', None)
    table_name = config.get('table_name', 'default_table')
    
    # Parse schema if provided
    schema = parse_schema(schema_str) if schema_str else None

    # Read data based on the file type
    if source_type == 'csv':
        df = glueContext.read.format("csv").option("header", "true").schema(schema).load(source_path) if schema else glueContext.read.format("csv").option("header", "true").load(source_path)
    elif source_type == 'parquet':
        df = glueContext.read.schema(schema).parquet(source_path) if schema else glueContext.read.parquet(source_path)
    elif source_type == 'json':
        df = glueContext.read.schema(schema).json(source_path) if schema else glueContext.read.json(source_path)
    else:
        raise ValueError(f"Unsupported source type: {source_type}")
    
    df.createOrReplaceTempView(table_name)
    return df

## @params: [JOB_NAME, CONFIG_FILE, OUTPUT_PATH, TABLE_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'CONFIG_FILE', 'OUTPUT_PATH', 'TABLE_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Load configuration
config = load_config(args['CONFIG_FILE'])

# Read data
df = read_data(glueContext, config)
df.show()

# Write data to output path
output_path = f"{args['OUTPUT_PATH'].rstrip('/')}/{args['TABLE_NAME']}"
df.write.mode("overwrite").parquet(output_path)


job.commit()
